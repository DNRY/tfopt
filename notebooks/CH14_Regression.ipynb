{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8276dd45",
   "metadata": {},
   "source": [
    "# 14장. 오버피팅의 진단과 해결책"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51367c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adcf646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터 생성\n",
    "np.random.seed(327)\n",
    "x_train = np.linspace(-1,1,50)\n",
    "y_train = x_train**3 + 0.1 * x_train**2 -0.15 * x_train + 1.0 + 0.5 * np.random.rand(len(x_train))\n",
    "\n",
    "# 테스트용 데이터 생성\n",
    "x_test = np.linspace(-1,1,50)\n",
    "y_test = x_test**3 + 0.1 * x_test**2 -0.15 * x_test + 1.0 + 0.5 * np.random.rand(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a3902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성값\n",
    "features_train = np.array([[xval] for xval in x_train])\n",
    "labels_train = y_train.reshape(-1, 1)\n",
    "\n",
    "features_test = np.array([[xval] for xval in x_test])\n",
    "labels_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        num_hidden1 = 10 \n",
    "        num_hidden2 = 5\n",
    "\n",
    "        self.W1 = tf.Variable(tf.random.normal([1, num_hidden1], seed=624)) \n",
    "        self.b1 = tf.Variable(tf.random.normal([num_hidden1], seed=624)) \n",
    "\n",
    "        self.W2 = tf.Variable(tf.random.normal([num_hidden1, num_hidden2], seed=624)) \n",
    "        self.b2 = tf.Variable(tf.random.normal([num_hidden2], seed=624)) \n",
    "\n",
    "        self.W3 = tf.Variable(tf.random.normal([num_hidden2, 1], seed=624)) \n",
    "        self.b3 = tf.Variable(tf.random.normal([1], seed=624)) \n",
    "\n",
    "    def call(self, x):\n",
    "        l1 = tf.matmul(x, self.W1) + self.b1\n",
    "        h1 = tf.nn.sigmoid(l1)\n",
    "\n",
    "        l2 = tf.matmul(h1, self.W2) + self.b2\n",
    "        h2 = tf.nn.sigmoid(l2)\n",
    "\n",
    "        yhat = tf.matmul(h2, self.W3) + self.b3\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e2f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "model = MyModel()\n",
    "\n",
    "# 손실함수\n",
    "loss = lambda y, yhat: tf.reduce_mean(tf.square( yhat - y ))\n",
    "\n",
    "# 수치최적화 알고리즘\n",
    "lr = 0.05\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "np.random.seed(320)\n",
    "shuffled_id = np.arange(0, len(x_train))\n",
    "np.random.shuffle(shuffled_id)\n",
    "shuffled_x_train = features_train[shuffled_id]\n",
    "shuffled_y_train = labels_train[shuffled_id]\n",
    "\n",
    "# 손실함수 기록용 변수\n",
    "loss_train_history = []\n",
    "loss_test_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2344c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 스토캐스틱 방법\n",
    "MaxEpochs = 10000\n",
    "batch_size = 10\n",
    "\n",
    "from helper import generate_batches\n",
    "\n",
    "for epoch in range(MaxEpochs):\n",
    "    if epoch % 100 == 0:\n",
    "        curr_loss = loss(labels_train, model(features_train))\n",
    "        print(epoch, curr_loss.numpy())\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            curr_loss = loss(y_batch, model(x_batch))\n",
    "            gradients = tape.gradient(curr_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    loss_train = loss(labels_train, model(features_train))\n",
    "    loss_test = loss(labels_test, model(features_test))\n",
    "    loss_train_history.append(loss_train)\n",
    "    loss_test_history.append(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_l2(pred, x_train, y_train, x_test, y_test):\n",
    "    from matplotlib.patches import Rectangle\n",
    "    plt.plot(x_train, y_train, '.k', markersize=7, markerfacecolor='none', label='Train Data')\n",
    "    plt.plot(x_train, pred,  '--k', label='Model')\n",
    "    plt.plot(x_test, y_test, 'xk', markersize=4, markerfacecolor='none', label='Test Data')\n",
    "    currentAxis = plt.gca()\n",
    "    for k, (xx,yy) in enumerate(zip(x_train, y_train)):\n",
    "        error = pred[k] - yy\n",
    "        currentAxis.add_patch(Rectangle((xx, yy), \n",
    "                                        error, error,\n",
    "                              alpha=0.1, facecolor='gray', edgecolor='k'))\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values = model(features_train)\n",
    "final_loss = loss(labels_train, prediction_values)\n",
    "prediction_values_test = model(features_test)\n",
    "final_loss_test = loss(labels_test, prediction_values_test)\n",
    "loss_train_history.append(final_loss)\n",
    "loss_test_history.append(final_loss_test)\n",
    "\n",
    "plt.title(\"Train Loss = {:1.3f}, Test Loss = {:1.3f}\\n at Epoch {}\".format(final_loss, final_loss_test, MaxEpochs))\n",
    "visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69bb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_train_history, '-k', label='Train', alpha=0.7)\n",
    "plt.plot(loss_test_history, '--', color='gray', label='Test', linewidth=2, alpha=0.7)\n",
    "plt.legend()\n",
    "ind = [50,200,1000,2000,3000,4000,6000,8000,10000]\n",
    "for j in ind:\n",
    "    plt.plot(j,loss_train_history[j].numpy(), 'k*', markersize=7)\n",
    "    plt.plot(j,loss_test_history[j].numpy(), '*', color='gray', markersize=7)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([-.01, 0.07])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d8cad",
   "metadata": {},
   "source": [
    "## 14.1 학습 반복 횟수 재설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        num_hidden1 = 10 \n",
    "        num_hidden2 = 5\n",
    "\n",
    "        self.W1 = tf.Variable(tf.random.normal([1, num_hidden1], seed=624)) \n",
    "        self.b1 = tf.Variable(tf.random.normal([num_hidden1], seed=624)) \n",
    "\n",
    "        self.W2 = tf.Variable(tf.random.normal([num_hidden1, num_hidden2], seed=624)) \n",
    "        self.b2 = tf.Variable(tf.random.normal([num_hidden2], seed=624)) \n",
    "\n",
    "        self.W3 = tf.Variable(tf.random.normal([num_hidden2, 1], seed=624)) \n",
    "        self.b3 = tf.Variable(tf.random.normal([1], seed=624)) \n",
    "\n",
    "    def call(self, x):\n",
    "        l1 = tf.matmul(x, self.W1) + self.b1\n",
    "        h1 = tf.nn.sigmoid(l1)\n",
    "\n",
    "        l2 = tf.matmul(h1, self.W2) + self.b2\n",
    "        h2 = tf.nn.sigmoid(l2)\n",
    "\n",
    "        yhat = tf.matmul(h2, self.W3) + self.b3\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "model = MyModel()\n",
    "\n",
    "# 수치최적화 알고리즘\n",
    "lr = 0.05\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "np.random.seed(320)\n",
    "shuffled_id = np.arange(0, len(x_train))\n",
    "np.random.shuffle(shuffled_id)\n",
    "shuffled_x_train = features_train[shuffled_id]\n",
    "shuffled_y_train = labels_train[shuffled_id]\n",
    "\n",
    "# 손실함수 기록용 변수\n",
    "loss_train_history = []\n",
    "loss_test_history = []\n",
    "\n",
    "# 스토캐스틱 방법\n",
    "MaxEpochs = 1000\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(MaxEpochs):\n",
    "    if epoch % 100 == 0:\n",
    "        curr_loss = loss(labels_train, model(features_train))\n",
    "        print(epoch, curr_loss.numpy())\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            curr_loss = loss(y_batch, model(x_batch))\n",
    "            gradients = tape.gradient(curr_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    loss_train = loss(labels_train, model(features_train))\n",
    "    loss_test = loss(labels_test, model(features_test))\n",
    "    loss_train_history.append(loss_train)\n",
    "    loss_test_history.append(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5589e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values = model(features_train)\n",
    "final_loss = loss(labels_train, prediction_values)\n",
    "prediction_values_test = model(features_test)\n",
    "final_loss_test = loss(labels_test, prediction_values_test)\n",
    "loss_train_history.append(final_loss)\n",
    "loss_test_history.append(final_loss_test)\n",
    "\n",
    "plt.title(\"Train Loss = {:1.3f}, Test Loss = {:1.3f}\\n at Epoch {}\".format(final_loss, final_loss_test, MaxEpochs))\n",
    "visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907d8bd",
   "metadata": {},
   "source": [
    "## 14.2 Regularization 함수 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388a3ab",
   "metadata": {},
   "source": [
    "### 14.2.1 $L^2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(327)\n",
    "x_train = np.linspace(-1,1,50)\n",
    "y_train = x_train**3 + 0.1 * x_train**2 -0.15 * x_train + 1.0 + 0.5 * np.random.rand(len(x_train))\n",
    "\n",
    "x_test = np.linspace(-1,1,50)\n",
    "y_test = x_test**3 + 0.1 * x_test**2 -0.15 * x_test + 1.0 + 0.5 * np.random.rand(len(x_test))\n",
    "\n",
    "features_train = np.array([[xval] for xval in x_train])\n",
    "labels_train = y_train.reshape(-1, 1)\n",
    "\n",
    "features_test = np.array([[xval] for xval in x_test])\n",
    "labels_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        num_hidden1 = 10 \n",
    "        num_hidden2 = 5\n",
    "\n",
    "        self.W1 = tf.Variable(tf.random.normal([1, num_hidden1], seed=624)) \n",
    "        self.b1 = tf.Variable(tf.random.normal([num_hidden1], seed=624)) \n",
    "\n",
    "        self.W2 = tf.Variable(tf.random.normal([num_hidden1, num_hidden2], seed=624)) \n",
    "        self.b2 = tf.Variable(tf.random.normal([num_hidden2], seed=624)) \n",
    "\n",
    "        self.W3 = tf.Variable(tf.random.normal([num_hidden2, 1], seed=624)) \n",
    "        self.b3 = tf.Variable(tf.random.normal([1], seed=624)) \n",
    "\n",
    "    def call(self, x):\n",
    "        l1 = tf.matmul(x, self.W1) + self.b1\n",
    "        h1 = tf.nn.sigmoid(l1)\n",
    "\n",
    "        l2 = tf.matmul(h1, self.W2) + self.b2\n",
    "        h2 = tf.nn.sigmoid(l2)\n",
    "\n",
    "        yhat = tf.matmul(h2, self.W3) + self.b3\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "l2_norm = lambda W1, W2, W3: tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8dea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxEpochs = 10000\n",
    "batch_size = 10\n",
    "lr = 0.05\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "beta = 1E-5\n",
    "loss = lambda y, yhat, W1, W2, W3: tf.reduce_mean(tf.square( yhat - y )) + beta*(tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3))\n",
    "mse = lambda y, yhat: tf.reduce_mean(tf.square( yhat - y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(320)\n",
    "shuffled_id = np.arange(0, len(x_train))\n",
    "np.random.shuffle(shuffled_id)\n",
    "shuffled_x_train = features_train[shuffled_id]\n",
    "shuffled_y_train = labels_train[shuffled_id]\n",
    "\n",
    "loss_train_history = []\n",
    "loss_test_history = []\n",
    "mse_train_history = []\n",
    "mse_test_history = []\n",
    "l2_norm_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b21e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(MaxEpochs):\n",
    "    loss_train = loss(labels_train, model(features_train), model.W1, model.W2, model.W3)\n",
    "    loss_test = loss(labels_test, model(features_test), model.W1, model.W2, model.W3)\n",
    "    loss_train_history.append(loss_train)\n",
    "    loss_test_history.append(loss_test)\n",
    "    mse_train_val = mse(labels_train, model(features_train))\n",
    "    mse_train_history.append(mse_train_val)\n",
    "    mse_test_val = mse(labels_test, model(features_test))\n",
    "    mse_test_history.append(mse_test_val)\n",
    "    l2_norm_val = l2_norm(model.W1, model.W2, model.W3)\n",
    "    l2_norm_history.append(l2_norm_val)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        curr_loss = loss(labels_train, model(features_train), model.W1, model.W2, model.W3)\n",
    "        print(epoch, curr_loss.numpy())\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            curr_loss = loss(y_batch, model(x_batch), model.W1, model.W2, model.W3)\n",
    "            gradients = tape.gradient(curr_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5162d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values = model(features_train)\n",
    "final_loss = loss(labels_train, prediction_values, model.W1, model.W2, model.W3)\n",
    "prediction_values_test = model(features_test)\n",
    "final_loss_test = loss(labels_test, prediction_values_test, model.W1, model.W2, model.W3)\n",
    "loss_train_history.append(final_loss)\n",
    "loss_test_history.append(final_loss_test)\n",
    "l2_norm_val = l2_norm(model.W1, model.W2, model.W3)\n",
    "mse_train_val = mse(labels_train, model(features_train))\n",
    "mse_train_history.append(mse_train_val)\n",
    "mse_test_val = mse(labels_test, model(features_test))\n",
    "mse_test_history.append(mse_test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"L2 norm = {:1.2f} at Epoch {}\".format(l2_norm_val, MaxEpochs))\n",
    "visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c354dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_train_history, '-k', label='Train', alpha=0.7)\n",
    "plt.plot(mse_test_history, '--', color='gray', label='Test', linewidth=2, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.ylim([-.01, 0.07])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l2_norm_history,'-k')\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('$L^2$-norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a60f05d",
   "metadata": {},
   "source": [
    "#### Dense( ) 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1be393",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(327)\n",
    "x_train = np.linspace(-1,1,50)\n",
    "y_train = x_train**3 + 0.1 * x_train**2 -0.15 * x_train + 1.0 + 0.5 * np.random.rand(len(x_train))\n",
    "\n",
    "x_test = np.linspace(-1,1,50)\n",
    "y_test = x_test**3 + 0.1 * x_test**2 -0.15 * x_test + 1.0 + 0.5 * np.random.rand(len(x_test))\n",
    "\n",
    "features_train = np.array([[xval] for xval in x_train])\n",
    "labels_train = y_train.reshape(-1, 1)\n",
    "\n",
    "features_test = np.array([[xval] for xval in x_test])\n",
    "labels_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031be975",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden1 = 10 \n",
    "num_hidden2 = 5\n",
    "W1 = tf.constant_initializer(tf.Variable(tf.random.normal([1, num_hidden1], seed=624)).numpy())\n",
    "b1 = tf.constant_initializer(tf.Variable(tf.random.normal([num_hidden1], seed=624)).numpy()) \n",
    "\n",
    "W2 = tf.constant_initializer(tf.Variable(tf.random.normal([num_hidden1, num_hidden2], seed=624)).numpy())\n",
    "b2 = tf.constant_initializer(tf.Variable(tf.random.normal([num_hidden2], seed=624)).numpy())\n",
    "\n",
    "W3 = tf.constant_initializer(tf.Variable(tf.random.normal([num_hidden2, 1], seed=624)).numpy())\n",
    "b3 = tf.constant_initializer(tf.Variable(tf.random.normal([1], seed=624)).numpy())\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        num_hidden1 = 10 \n",
    "        num_hidden2 = 5\n",
    "        beta = 1E-5/2\n",
    "\n",
    "        self.hidden1 = tf.keras.layers.Dense(num_hidden1, activation=tf.nn.sigmoid, \n",
    "            kernel_initializer=W1, bias_initializer=b1, name='hidden_1', \n",
    "            kernel_regularizer=tf.keras.regularizers.L2(beta))\n",
    "\n",
    "        self.hidden2 = tf.keras.layers.Dense(num_hidden2, activation=tf.nn.sigmoid, \n",
    "            kernel_initializer=W2, bias_initializer=b2, name='hidden_2', \n",
    "            kernel_regularizer=tf.keras.regularizers.L2(beta))\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(1, activation = None, \n",
    "            kernel_initializer=W3, bias_initializer=b3, name='output', \n",
    "            kernel_regularizer=tf.keras.regularizers.L2(beta))\n",
    "\n",
    "    def call(self, x):\n",
    "        h1 = self.hidden1(x)\n",
    "        h2 = self.hidden2(h1)\n",
    "        yhat = self.out(h2)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "MaxEpochs = 10000\n",
    "batch_size = 10\n",
    "lr = 0.05\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "np.random.seed(320)\n",
    "shuffled_id = np.arange(0, len(x_train))\n",
    "np.random.shuffle(shuffled_id)\n",
    "shuffled_x_train = features_train[shuffled_id]\n",
    "shuffled_y_train = labels_train[shuffled_id]\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = loss)\n",
    "history = model.fit(shuffled_x_train, shuffled_y_train, epochs = MaxEpochs, batch_size = batch_size, \n",
    "                    shuffle = False, validation_data = (features_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e52ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm = lambda W1, W2, W3: tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3)\n",
    "l2_norm_val = l2_norm(model.layers[0].get_weights()[0], model.layers[1].get_weights()[0], model.layers[2].get_weights()[0])\n",
    "\n",
    "prediction_values = model(features_train)\n",
    "final_loss = model.evaluate(features_train,labels_train)\n",
    "prediction_values_test = model(features_test)\n",
    "final_loss_test = model.evaluate(features_test,labels_test)\n",
    "\n",
    "mse = lambda y, yhat: tf.reduce_mean(tf.square( yhat - y ))\n",
    "mse_train_val = mse(labels_train, model(features_train))\n",
    "mse_test_val = mse(labels_test, model(features_test))\n",
    "beta = 1E-5\n",
    "print('mse:',mse_train_val.numpy(), mse_test_val.numpy())\n",
    "print('l2_norm:', l2_norm_val.numpy())\n",
    "print('loss:', mse_train_val.numpy() + beta*l2_norm_val.numpy(), mse_test_val.numpy() + beta*l2_norm_val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Train Loss = {:1.3f}, Test Loss = {:1.3f}\\n at Epoch {}\".format(final_loss, final_loss_test, MaxEpochs))\n",
    "visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65095361",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"L2 norm = {:1.2f} at Epoch {}\".format(l2_norm_val, MaxEpochs))\n",
    "visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], '-k', label='Train', alpha=0.7)\n",
    "plt.plot(history.history['val_loss'], '--', color='gray', label='Test', linewidth=2, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([-.01, 0.07])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b404df8",
   "metadata": {},
   "source": [
    "### 14.2.2 $L^1$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77635762",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(327)\n",
    "x_train = np.linspace(-1,1,50)\n",
    "y_train = x_train**3 + 0.1 * x_train**2 -0.15 * x_train + 1.0 + 0.5 * np.random.rand(len(x_train))\n",
    "\n",
    "x_test = np.linspace(-1,1,50)\n",
    "y_test = x_test**3 + 0.1 * x_test**2 -0.15 * x_test + 1.0 + 0.5 * np.random.rand(len(x_test))\n",
    "\n",
    "features_train = np.array([[xval] for xval in x_train])\n",
    "labels_train = y_train.reshape(-1, 1)\n",
    "\n",
    "features_test = np.array([[xval] for xval in x_test])\n",
    "labels_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        num_hidden1 = 10 \n",
    "        num_hidden2 = 5\n",
    "\n",
    "        self.W1 = tf.Variable(tf.random.normal([1, num_hidden1], seed=624)) \n",
    "        self.b1 = tf.Variable(tf.random.normal([num_hidden1], seed=624)) \n",
    "\n",
    "        self.W2 = tf.Variable(tf.random.normal([num_hidden1, num_hidden2], seed=624)) \n",
    "        self.b2 = tf.Variable(tf.random.normal([num_hidden2], seed=624)) \n",
    "\n",
    "        self.W3 = tf.Variable(tf.random.normal([num_hidden2, 1], seed=624)) \n",
    "        self.b3 = tf.Variable(tf.random.normal([1], seed=624)) \n",
    "\n",
    "    def call(self, x):\n",
    "        l1 = tf.matmul(x, self.W1) + self.b1\n",
    "        h1 = tf.nn.sigmoid(l1)\n",
    "\n",
    "        l2 = tf.matmul(h1, self.W2) + self.b2\n",
    "        h2 = tf.nn.sigmoid(l2)\n",
    "\n",
    "        yhat = tf.matmul(h2, self.W3) + self.b3\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df65fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "l1_norm = lambda W1, W2, W3: tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2)) + tf.reduce_sum(tf.abs(W3))\n",
    "\n",
    "MaxEpochs = 10000\n",
    "batch_size = 10\n",
    "lr = 0.05\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "beta = 1E-4\n",
    "loss = lambda y, yhat, W1, W2, W3: tf.reduce_mean(tf.square( yhat - y )) + beta*(tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2)) + tf.reduce_sum(tf.abs(W3)))\n",
    "mse = lambda y, yhat: tf.reduce_mean(tf.square( yhat - y ))\n",
    "\n",
    "np.random.seed(320)\n",
    "shuffled_id = np.arange(0, len(x_train))\n",
    "np.random.shuffle(shuffled_id)\n",
    "shuffled_x_train = features_train[shuffled_id]\n",
    "shuffled_y_train = labels_train[shuffled_id]\n",
    "\n",
    "loss_train_history = []\n",
    "loss_test_history = []\n",
    "mse_train_history = []\n",
    "mse_test_history = []\n",
    "l1_norm_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(MaxEpochs):\n",
    "    loss_train = loss(labels_train, model(features_train), model.W1, model.W2, model.W3)\n",
    "    loss_test = loss(labels_test, model(features_test), model.W1, model.W2, model.W3)\n",
    "    loss_train_history.append(loss_train)\n",
    "    loss_test_history.append(loss_test)\n",
    "    mse_train_val = mse(labels_train, model(features_train))\n",
    "    mse_train_history.append(mse_train_val)\n",
    "    mse_test_val = mse(labels_test, model(features_test))\n",
    "    mse_test_history.append(mse_test_val)\n",
    "    l1_norm_val = l1_norm(model.W1, model.W2, model.W3)\n",
    "    l1_norm_history.append(l1_norm_val)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        curr_loss = loss(labels_train, model(features_train), model.W1, model.W2, model.W3)\n",
    "        print(epoch, curr_loss.numpy())\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            curr_loss = loss(y_batch, model(x_batch), model.W1, model.W2, model.W3)\n",
    "            gradients = tape.gradient(curr_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    if epoch in [50, 1000, 4000, 8000]:\n",
    "        prediction_values = model(features_train)\n",
    "        l1_norm_val = l1_norm(model.W1, model.W2, model.W3)\n",
    "        plt.title(\"L1 norm = {:1.2f} at Epoch {}\".format(l1_norm_val, epoch))\n",
    "        visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values = model(features_train)\n",
    "final_loss = loss(labels_train, prediction_values, model.W1, model.W2, model.W3)\n",
    "prediction_values_test = model(features_test)\n",
    "final_loss_test = loss(labels_test, prediction_values_test, model.W1, model.W2, model.W3)\n",
    "loss_train_history.append(final_loss)\n",
    "loss_test_history.append(final_loss_test)\n",
    "l1_norm_val = l1_norm(model.W1, model.W2, model.W3)\n",
    "mse_train_val = mse(labels_train, model(features_train))\n",
    "mse_train_history.append(mse_train_val)\n",
    "mse_test_val = mse(labels_test, model(features_test))\n",
    "mse_test_history.append(mse_test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"L1 norm = {:1.2f} at Epoch {}\".format(l1_norm_val, MaxEpochs))\n",
    "visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82856f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_train_history, '-k', label='Train', alpha=0.7)\n",
    "plt.plot(mse_test_history, '--', color='gray', label='Test', linewidth=2, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.ylim([-.01, 0.07])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l1_norm_history,'-k')\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('$L^1$-norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424e9f3",
   "metadata": {},
   "source": [
    "#### Dense( ) 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(327)\n",
    "x_train = np.linspace(-1,1,50)\n",
    "y_train = x_train**3 + 0.1 * x_train**2 -0.15 * x_train + 1.0 + 0.5 * np.random.rand(len(x_train))\n",
    "\n",
    "x_test = np.linspace(-1,1,50)\n",
    "y_test = x_test**3 + 0.1 * x_test**2 -0.15 * x_test + 1.0 + 0.5 * np.random.rand(len(x_test))\n",
    "\n",
    "features_train = np.array([[xval] for xval in x_train])\n",
    "labels_train = y_train.reshape(-1, 1)\n",
    "\n",
    "features_test = np.array([[xval] for xval in x_test])\n",
    "labels_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d014c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden1 = 10 \n",
    "num_hidden2 = 5\n",
    "W1 = tf.constant_initializer(tf.Variable(tf.random.normal([1, num_hidden1], seed=624)).numpy())\n",
    "b1 = tf.constant_initializer(tf.Variable(tf.random.normal([num_hidden1], seed=624)).numpy()) \n",
    "\n",
    "W2 = tf.constant_initializer(tf.Variable(tf.random.normal([num_hidden1, num_hidden2], seed=624)).numpy())\n",
    "b2 = tf.constant_initializer(tf.Variable(tf.random.normal([num_hidden2], seed=624)).numpy())\n",
    "\n",
    "W3 = tf.constant_initializer(tf.Variable(tf.random.normal([num_hidden2, 1], seed=624)).numpy())\n",
    "b3 = tf.constant_initializer(tf.Variable(tf.random.normal([1], seed=624)).numpy())\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        num_hidden1 = 10 \n",
    "        num_hidden2 = 5\n",
    "        beta = 1E-4\n",
    "\n",
    "        self.hidden1 = tf.keras.layers.Dense(num_hidden1, activation=tf.nn.sigmoid, \n",
    "            kernel_initializer=W1, bias_initializer=b1, name='hidden_1', \n",
    "            kernel_regularizer=tf.keras.regularizers.L1(beta))\n",
    "\n",
    "        self.hidden2 = tf.keras.layers.Dense(num_hidden2, activation=tf.nn.sigmoid, \n",
    "            kernel_initializer=W2, bias_initializer=b2, name='hidden_2', \n",
    "            kernel_regularizer=tf.keras.regularizers.L1(beta))\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(1, activation = None, \n",
    "            kernel_initializer=W3, bias_initializer=b3, name='output', \n",
    "            kernel_regularizer=tf.keras.regularizers.L1(beta))\n",
    "\n",
    "    def call(self, x):\n",
    "        h1 = self.hidden1(x)\n",
    "        h2 = self.hidden2(h1)\n",
    "        yhat = self.out(h2)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "MaxEpochs = 10000\n",
    "batch_size = 10\n",
    "lr = 0.05\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "np.random.seed(320)\n",
    "shuffled_id = np.arange(0, len(x_train))\n",
    "np.random.shuffle(shuffled_id)\n",
    "shuffled_x_train = features_train[shuffled_id]\n",
    "shuffled_y_train = labels_train[shuffled_id]\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = loss)\n",
    "history = model.fit(shuffled_x_train, shuffled_y_train, epochs = MaxEpochs, batch_size = batch_size, \n",
    "                    shuffle = False, validation_data = (features_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_norm = lambda W1, W2, W3: tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2)) + tf.reduce_sum(tf.abs(W3))\n",
    "l1_norm_val = l1_norm(model.layers[0].get_weights()[0], model.layers[1].get_weights()[0], model.layers[2].get_weights()[0])\n",
    "\n",
    "prediction_values = model(features_train)\n",
    "final_loss = model.evaluate(features_train,labels_train)\n",
    "prediction_values_test = model(features_test)\n",
    "final_loss_test = model.evaluate(features_test,labels_test)\n",
    "\n",
    "mse = lambda y, yhat: tf.reduce_mean(tf.square( yhat - y ))\n",
    "mse_train_val = mse(labels_train, model(features_train))\n",
    "mse_test_val = mse(labels_test, model(features_test))\n",
    "beta = 1E-4\n",
    "print('mse:',mse_train_val.numpy(), mse_test_val.numpy())\n",
    "print('l1_norm:', l1_norm_val.numpy())\n",
    "print('loss:', mse_train_val.numpy() + beta*l1_norm_val.numpy(), mse_test_val.numpy() + beta*l1_norm_val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3273f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Train Loss = {:1.3f}, Test Loss = {:1.3f}\\n at Epoch {}\".format(final_loss, final_loss_test, MaxEpochs))\n",
    "visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"L1 norm = {:1.2f} at Epoch {}\".format(l1_norm_val, MaxEpochs))\n",
    "visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e06197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], '-k', label='Train', alpha=0.7)\n",
    "plt.plot(history.history['val_loss'], '--', color='gray', label='Test', linewidth=2, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([-.01, 0.07])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ce744",
   "metadata": {},
   "source": [
    "## 14.3 드롭아웃(Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "data = np.arange(1,11).astype(np.float32)\n",
    "layer = tf.keras.layers.Dropout(rate = .5, input_shape = (10,))\n",
    "\n",
    "outputs = layer(data, training = True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74058412",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dropout(rate = .9, input_shape = (10,))\n",
    "outputs = layer(data, training = True)\n",
    "print(outputs)\n",
    "\n",
    "layer = tf.keras.layers.Dropout(rate = .8, input_shape = (10,))\n",
    "outputs = layer(data, training = True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = layer(data)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "np.random.seed(327)\n",
    "x_train = np.linspace(-1,1,50)\n",
    "y_train = x_train**3 + 0.1 * x_train**2 -0.15 * x_train + 1.0 + 0.5 * np.random.rand(len(x_train))\n",
    "\n",
    "x_test = np.linspace(-1,1,50)\n",
    "y_test = x_test**3 + 0.1 * x_test**2 -0.15 * x_test + 1.0 + 0.5 * np.random.rand(len(x_test))\n",
    "\n",
    "features_train = np.array([[xval] for xval in x_train])\n",
    "labels_train = y_train.reshape(-1, 1)\n",
    "\n",
    "features_test = np.array([[xval] for xval in x_test])\n",
    "labels_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d318f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        num_hidden1 = 10 \n",
    "        num_hidden2 = 5\n",
    "\n",
    "        self.W1 = tf.Variable(tf.random.normal([1, num_hidden1], seed=119)) \n",
    "        self.b1 = tf.Variable(tf.random.normal([num_hidden1], seed=119)) \n",
    "\n",
    "        self.W2 = tf.Variable(tf.random.normal([num_hidden1, num_hidden2], seed=119)) \n",
    "        self.b2 = tf.Variable(tf.random.normal([num_hidden2], seed=119)) \n",
    "\n",
    "        self.W3 = tf.Variable(tf.random.normal([num_hidden2, 1], seed=119)) \n",
    "        self.b3 = tf.Variable(tf.random.normal([1], seed=119)) \n",
    "\n",
    "        self.Dropout_layer = tf.keras.layers.Dropout(rate = .15, seed = 1200)\n",
    "\n",
    "    def call(self, x, training = False):\n",
    "        l1 = tf.matmul(x, self.W1) + self.b1\n",
    "        h1 = tf.nn.sigmoid(l1)\n",
    "        h1 = self.Dropout_layer(h1, training = training)\n",
    "\n",
    "        l2 = tf.matmul(h1, self.W2) + self.b2\n",
    "        h2 = tf.nn.sigmoid(l2)\n",
    "        h2 = self.Dropout_layer(h2, training = training)\n",
    "\n",
    "        yhat = tf.matmul(h2, self.W3) + self.b3\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "MaxEpochs = 10000\n",
    "batch_size = 10\n",
    "lr = 0.05\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "loss = lambda y, yhat: tf.reduce_mean(tf.square( yhat - y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5345d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 섞기\n",
    "np.random.seed(320)\n",
    "shuffled_id = np.arange(0, len(x_train))\n",
    "np.random.shuffle(shuffled_id)\n",
    "shuffled_x_train = features_train[shuffled_id]\n",
    "shuffled_y_train = labels_train[shuffled_id]\n",
    "\n",
    "# 손실함수 기록용 변수\n",
    "loss_train_history = []\n",
    "loss_test_history = []\n",
    "\n",
    "# 스토캐스틱 방법\n",
    "from helper import generate_batches\n",
    "for epoch in range(MaxEpochs):\n",
    "    if epoch % 100 == 0:\n",
    "        curr_loss = loss(labels_train, model(features_train))\n",
    "        print(epoch, curr_loss.numpy())\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            curr_loss = loss(y_batch, model(x_batch, training = True))\n",
    "            gradients = tape.gradient(curr_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    loss_train = loss(labels_train, model(features_train))\n",
    "    loss_test = loss(labels_test, model(features_test))\n",
    "    loss_train_history.append(loss_train)\n",
    "    loss_test_history.append(loss_test)\n",
    "\n",
    "    if (epoch+1) in [1000,4000,8000]:\n",
    "        prediction_values = model(features_train)\n",
    "        final_loss = loss(labels_train, prediction_values)\n",
    "        prediction_values_test = model(features_test)\n",
    "        final_loss_test = loss(labels_test, prediction_values_test)\n",
    "        plt.title(\"Train Loss = {:1.3f}, Test Loss = {:1.3f}\\n at Epoch {}\".format(final_loss, final_loss_test, epoch+1))\n",
    "        visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73745c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values = model(features_train)\n",
    "final_loss = loss(labels_train, prediction_values)\n",
    "prediction_values_test = model(features_test)\n",
    "final_loss_test = loss(labels_test, prediction_values_test)\n",
    "loss_train_history.append(final_loss)\n",
    "loss_test_history.append(final_loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4338f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Train Loss = {:1.3f}, Test Loss = {:1.3f}\\n at Epoch {}\".format(final_loss, final_loss_test, MaxEpochs))\n",
    "visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae560c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_train_history, '-k', label='Train', alpha=0.7)\n",
    "plt.plot(loss_test_history, '--', color='gray', label='Test', linewidth=2, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([-.01, 0.07])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c9944c",
   "metadata": {},
   "source": [
    "#### Dense( ) 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4923af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(327)\n",
    "x_train = np.linspace(-1,1,50)\n",
    "y_train = x_train**3 + 0.1 * x_train**2 -0.15 * x_train + 1.0 + 0.5 * np.random.rand(len(x_train))\n",
    "\n",
    "x_test = np.linspace(-1,1,50)\n",
    "y_test = x_test**3 + 0.1 * x_test**2 -0.15 * x_test + 1.0 + 0.5 * np.random.rand(len(x_test))\n",
    "\n",
    "features_train = np.array([[xval] for xval in x_train])\n",
    "labels_train = y_train.reshape(-1, 1)\n",
    "\n",
    "features_test = np.array([[xval] for xval in x_test])\n",
    "labels_test = y_test.reshape(-1, 1)\n",
    "\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1., seed=119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78fbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        num_hidden1 = 10 \n",
    "        num_hidden2 = 5\n",
    "\n",
    "        self.hidden1 = tf.keras.layers.Dense(num_hidden1, activation=tf.nn.sigmoid, \n",
    "            kernel_initializer=initializer, bias_initializer=initializer, name='hidden_1')\n",
    "\n",
    "        self.hidden2 = tf.keras.layers.Dense(num_hidden2, activation=tf.nn.sigmoid, \n",
    "            kernel_initializer=initializer, bias_initializer=initializer, name='hidden_2')\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(1, activation = None, \n",
    "            kernel_initializer=initializer, bias_initializer=initializer, name='output')\n",
    "\n",
    "        self.Dropout_layer = tf.keras.layers.Dropout(rate = .1, seed = 1200)\n",
    "\n",
    "    def call(self, x, training = False):\n",
    "        h1 = self.hidden1(x)\n",
    "        h1 = self.Dropout_layer(h1, training = training)\n",
    "        h2 = self.hidden2(h1)\n",
    "        h2 = self.Dropout_layer(h2, training = training)\n",
    "        yhat = self.out(h2)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "MaxEpochs = 10000\n",
    "batch_size = 10\n",
    "lr = 0.05\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# 데이터 섞기\n",
    "np.random.seed(320)\n",
    "shuffled_id = np.arange(0, len(x_train))\n",
    "np.random.shuffle(shuffled_id)\n",
    "shuffled_x_train = features_train[shuffled_id]\n",
    "shuffled_y_train = labels_train[shuffled_id]\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = loss)\n",
    "history = model.fit(shuffled_x_train, shuffled_y_train, epochs = MaxEpochs, batch_size = batch_size, \n",
    "                    shuffle = False, validation_data = (features_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23215d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values = model(features_train)\n",
    "final_loss = model.evaluate(features_train,labels_train)\n",
    "prediction_values_test = model(features_test)\n",
    "final_loss_test = model.evaluate(features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = lambda y, yhat: tf.reduce_mean(tf.square( yhat - y ))\n",
    "mse_train_val = mse(labels_train, model(features_train))\n",
    "mse_test_val = mse(labels_test, model(features_test))\n",
    "print('loss:', mse_train_val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd646944",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Train Loss = {:1.3f}, Test Loss = {:1.3f}\\n at Epoch {}\".format(final_loss, final_loss_test, MaxEpochs))\n",
    "visualize_l2(prediction_values.numpy().reshape(-1), x_train, y_train, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39020ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
